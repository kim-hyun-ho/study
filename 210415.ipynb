{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210415.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPG/jh75ZgHoNg/VSkYolWO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-hyun-ho/study/blob/main/210415.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5465IZ25-BXb"
      },
      "source": [
        "#엔트로피<p>\n",
        "$I_H=-\\sum_{j=1}^{c}p_j\\log{(p_j)}$\n",
        "<br><br>\n",
        "**노드 안에 들어있는 종류 (확률log2확률) 의 합**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL7A0ox4_HT7"
      },
      "source": [
        "#지니지수<p>\n",
        "$I_G = 1 - \\sum_{j=1}^c P_j^2 $\n",
        "<br><br>\n",
        "**1 - (노드 확률 제곱)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACZI3O8KA4qk"
      },
      "source": [
        "#불순도 지수를 지니지수를 쓸 때\n",
        "\n",
        "1. 분지 방법을 선택했을 때 새로운 T'의 불순도를 구한다.\n",
        "\n",
        "2. 불순도가 얼마나 감소하는지 측정 : I(T) - I(T')\n",
        "\n",
        "3. 불순도 감소량을 최대로하는 방법을 선택한다.\n",
        "\n",
        "https://m.blog.naver.com/ajh794/221730573964\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEYAwGRP1F5c"
      },
      "source": [
        "#PCA 주성분 분석\n",
        "* 차원을 줄이면서 점들이 퍼지게 == 분산이 커지는 방법으로 줄인다. <> 겹치면 정보가 유실된다.\n",
        "\n",
        "* Eigen Vector 추출\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnYPoA0c27UJ"
      },
      "source": [
        "## 유전 알고리즘을 이용한 여행하는 세일즈맨 문제\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pI9kJj_C7qmK"
      },
      "source": [
        "#짧은 문장은 RNN\n",
        "#긴문장은? LSTM\n",
        "* 그래디언트 배니싱\n",
        "* 그래디언트 익스플로딩\n",
        "\n",
        "#위 문제 극복위해 LSTM\n",
        "*Long Short-Term Memory\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTB8FHAAK7LI"
      },
      "source": [
        "#RNN 순환 신경망\n",
        "* 소프트맥스 인풋은 로짓 출력은 예측\n",
        "* bptt\n",
        "* 감정분석\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O41gvbCzO9r9"
      },
      "source": [
        "# 결정트리 종류\n",
        "* id3 : Entropy : 다지분리(범주)\n",
        "* C4.5 : Infomation Gain : 다지 및 이진(수치)\n",
        "* C5.0 : Information Gain : C4.5와 유사\n",
        "* CHAID : 카이제곱(범주),F검정(수치) : 통계적\n",
        "* CART : Gini Index(범주), 분산의 차이(수치) : 항상2진, 통계적\n",
        "\n",
        "#앙상블 (배깅=병렬, 부스팅=순차)\n",
        "\n",
        "## 약분류기 종류\n",
        "* stumps : depth가 1\n",
        "* shallow\n",
        "* Naive Bayes\n",
        "* Logistic Regression\n",
        "\n",
        "##랜덤 포레스트(다수결,배깅(중복 허용))\n",
        "* 작은트리 여러개\n",
        "\n",
        "\n",
        "## 중복 없는건 페이스팅# 중복 없는건 페이스팅\n",
        "\n",
        "##AdaBoost\n",
        "* https://www.youtube.com/watch?v=HZg8_wZPZGU\n",
        "* 약한 학습기(weak learner)의 상보 효과 성능향상.\n",
        "* 앞단 약분류리 보완을 위해 가중치를 갱신해서 다음 분류기 학습\n",
        "* 50~100개, yi={-,+1} 알고리즘상의 이유\n",
        "* 최소한 랜덤 찍기보다는 잘해라.\n",
        "* 오류율에 따라 가중치를 준다\n",
        "* 바르게 분류된건 다음단계에서 선택 안되고\n",
        "* 오분류가 다음단계에서 학습데이터로 선택된다.\n",
        "* 단계를 거듭할 수로 복잡한 분류 경계면이 만들어지고 잘 분류하게 된다.\n",
        "* 딥러닝 이전에 성능 속도 모두 좋아 얼굴인식에도 쓰였다.\n",
        "\n",
        "\n",
        "\n",
        "##배깅이 병렬이라 빠를거 같지만 실제 인공신경망 시간이 많이 걸려서 부스팅이 빨리 종료 될 수 있다.\n",
        "\n",
        "\n",
        "##GBM 이해\n",
        "* 순차적 : 앞 모델의 잔차를 맞춰라 > 잔차를 학습 데이터로 제공.\n",
        "\n",
        "* 로스펑션 > 로스펑션기울기(미분) 줄여라 => 경사하강\n",
        "* 약분류기를 로스가 영보다 크면 가중치를 그래디언트 반대 방향으로 욺직여라(조금씩)\n",
        "* 20번째 정도 하면 잔차가 거의 없이 줄어듬(잘맞춤)\n",
        "\n",
        "* 사용하는 로스펑션 : 제곱, 절대갑,huber, quantile\n",
        "\n",
        "* 분류에서 : 베르누이 로스, 에이다부스트 로스\n",
        "\n",
        "* 단점은? : 현실 노이즈를 다 맞추겠다는 관점 >> 과적합 현상 단점은? : 현실 노이즈를 다 맞추겠다는 관점 >> 과적합 현상 발생\n",
        "\n",
        "* 해결은? : 일반화 규제 추가 > 서브샘플링(앞단의 80%만 랜덤하게 샘플링(복원,비복원)) >>>> 잔차는 원데이터에서 구하므로 샘플링은 원데이터의 80%를 계속 뽑음.\n",
        "\n",
        "* 해결 > shrinkage : 두번째 학습부터 영향력 감소\n",
        "\n",
        "* Eary Stoping : 과적합 되기전에 끝냄.\n",
        "\n",
        "\n",
        "#랜덤 , GBM 비교\n",
        "* 성능 좋음, 변수 중요도(영향력) 평가 가능 \n",
        "\n",
        "https://www.youtube.com/watch?v=d6nRgztYWQM\n",
        "\n",
        "https://3months.tistory.com/368\n",
        "\n",
        "\n",
        "\n",
        "#숙제\n",
        "\n"
      ]
    }
  ]
}